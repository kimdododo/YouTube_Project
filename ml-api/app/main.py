from typing import Any, Dict, List
from fastapi import FastAPI
from pydantic import BaseModel
import numpy as np
import onnxruntime as ort
import os
import json
from typing import Optional, Dict

# í† í¬ë‚˜ì´ì € ë¡œë“œë¥¼ ìœ„í•œ import (ì„ íƒì )
try:
    from transformers import AutoTokenizer
    _has_transformers = True
except ImportError:
    _has_transformers = False
    print("âš ï¸ transformers not installed, ONNX inference will use fallback")


class EmbedIn(BaseModel):
    text: str


class EmbedBatchIn(BaseModel):
    texts: List[str]


class EmbedOut(BaseModel):
    embedding: List[float]
    dim: int


class SentimentIn(BaseModel):
    text: str


class SentimentOut(BaseModel):
    score: float


class RerankCandidate(BaseModel):
    id: str
    text: str


class RerankIn(BaseModel):
    query: str
    candidates: List[RerankCandidate]


class RerankOutItem(BaseModel):
    id: str
    score: float


app = FastAPI(title="ML API", version="0.1.0")

_ort_sess: Optional[ort.InferenceSession] = None
_tokenizer: Optional[Any] = None
_dim: int = int(os.getenv("EMBEDDING_DIM", "768"))
_embeddings_map: Optional[Dict[str, List[float]]] = None
_word_map: Optional[Dict[str, List[float]]] = None


@app.on_event("startup")
def startup_load_model() -> None:
    global _ort_sess, _tokenizer
    global _embeddings_map, _word_map, _dim
    
    # ONNX ëª¨ë¸ ë¡œë“œ (optional, ì—†ì–´ë„ ë¨)
    model_path = os.getenv("MODEL_PATH", "./model/finetuned_video_sts.onnx")
    if model_path and os.path.exists(model_path):
        try:
            providers = ["CPUExecutionProvider"]
            _ort_sess = ort.InferenceSession(model_path, providers=providers)
            print(f"âœ“ ONNX model loaded from {model_path}")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ (ONNX ëª¨ë¸ì´ ìžˆìœ¼ë©´ í† í¬ë‚˜ì´ì €ë„ í•„ìš”)
            if _has_transformers:
                tokenizer_path = os.path.dirname(model_path)  # ëª¨ë¸ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬
                try:
                    _tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)
                    print(f"âœ“ Tokenizer loaded from {tokenizer_path}")
                except Exception as e:
                    print(f"âš ï¸ Tokenizer load failed: {e}")
                    _tokenizer = None
        except Exception as e:
            print(f"âš ï¸ ONNX model load failed: {e}")
            _ort_sess = None
    else:
        _ort_sess = None
        print(f"â„¹ï¸ ONNX model not found at {model_path}, using JSON embeddings only")

    # JSON embeddings (ë¬¸ìž¥ ì „ì²´ ë§¤í•‘)
    emb_json = os.getenv("EMBEDDINGS_JSON", "./model/embeddings.json")
    if os.path.exists(emb_json):
        try:
            with open(emb_json, "r", encoding="utf-8") as f:
                _embeddings_map = json.load(f)
            # adjust dim if provided vectors indicate a size
            for _k, v in _embeddings_map.items():
                if isinstance(v, list) and len(v) > 0:
                    _dim = len(v)
                    break
            print(f"âœ“ Loaded {len(_embeddings_map)} embeddings from {emb_json}")
        except Exception as e:
            print(f"âš ï¸ Failed to load embeddings.json: {e}")
            _embeddings_map = None
    else:
        print(f"â„¹ï¸ embeddings.json not found at {emb_json}")

    # ë‹¨ì–´ ìž„ë² ë”© (í† í¬ë‚˜ì´ì € ë°©ì‹)
    word_json = os.getenv("WORD_EMBED_JSON", "./model/tokenizer/word_embeddings.json")
    if os.path.exists(word_json):
        try:
            with open(word_json, "r", encoding="utf-8") as f:
                _word_map = json.load(f)
            # adjust dim from first
            for _k, v in _word_map.items():
                if isinstance(v, list) and len(v) > 0:
                    _dim = len(v)
                    break
            print(f"âœ“ Loaded {len(_word_map)} word embeddings from {word_json}")
        except Exception as e:
            print(f"âš ï¸ Failed to load word_embeddings.json: {e}")
            _word_map = None
    else:
        print(f"â„¹ï¸ word_embeddings.json not found at {word_json}")
    
    print(f"ðŸ“Š Embedding dimension: {_dim}")


@app.get("/ping")
def ping() -> Dict[str, str]:
    return {"status": "ok"}


def _simple_encode(texts: List[str]) -> np.ndarray:
    # Priority 1: ONNX model inference
    if _ort_sess is not None and _tokenizer is not None:
        try:
            # í† í¬ë‚˜ì´ì €ë¡œ í…ìŠ¤íŠ¸ ì¸ì½”ë”©
            encoded = _tokenizer(texts, return_tensors="np", padding=True, truncation=True, max_length=512)
            input_ids = encoded["input_ids"].astype(np.int64)
            attention_mask = encoded["attention_mask"].astype(np.int64)
            
            # ONNX ì¶”ë¡ 
            outputs = _ort_sess.run(
                None,
                {
                    "input_ids": input_ids,
                    "attention_mask": attention_mask
                }
            )
            
            # last_hidden_stateì—ì„œ í‰ê·  pooling (ê°„ë‹¨í•œ ë°©ë²•)
            # outputs[0] = last_hidden_state [batch, seq_len, hidden_dim]
            # outputs[1] = pooler_output [batch, hidden_dim] (ìžˆìœ¼ë©´ ì‚¬ìš©)
            if len(outputs) > 1 and outputs[1] is not None:
                # pooler_output ì‚¬ìš©
                embeddings = outputs[1].astype(np.float32)
            else:
                # last_hidden_stateì—ì„œ attention_maskë¡œ í‰ê·  pooling
                last_hidden = outputs[0].astype(np.float32)  # [batch, seq, hidden]
                mask = attention_mask.astype(np.float32)[:, :, np.newaxis]  # [batch, seq, 1]
                masked_sum = (last_hidden * mask).sum(axis=1)  # [batch, hidden]
                mask_sum = mask.sum(axis=1)  # [batch, 1]
                embeddings = masked_sum / (mask_sum + 1e-9)  # [batch, hidden]
            
            # L2 ì •ê·œí™” (ì„ íƒì , ì¼ë°˜ì ìœ¼ë¡œ cosine similarityë¥¼ ìœ„í•´)
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
            embeddings = embeddings / (norms + 1e-9)
            
            return embeddings.astype(np.float32)
        except Exception as e:
            print(f"âš ï¸ ONNX ì¶”ë¡  ì‹¤íŒ¨: {e}")
            # fallbackìœ¼ë¡œ ê³„ì† ì§„í–‰
    
    # Priority 2: exact match from embeddings.json
    if _embeddings_map is not None and len(texts) == 1:
        vec = _embeddings_map.get(texts[0])
        if isinstance(vec, list) and len(vec) > 0:
            return np.array([vec], dtype=np.float32)

    # Priority 3: average word vectors from tokenizer/word_embeddings.json
    if _word_map is not None:
        vecs = []
        for t in texts:
            tokens = t.split()
            token_vecs = [np.array(_word_map[w], dtype=np.float32) for w in tokens if isinstance(_word_map.get(w), list)]
            if token_vecs:
                v = np.mean(token_vecs, axis=0)
            else:
                v = np.zeros((_dim,), dtype=np.float32)
            vecs.append(v)
        return np.stack(vecs, axis=0)

    # Priority 4: deterministic pseudo-embedding
    rng = np.random.default_rng(abs(hash("|".join(texts))) % (2**32))
    return rng.normal(size=(len(texts), _dim)).astype(np.float32)


@app.post("/embed", response_model=EmbedOut)
def embed(in_obj: EmbedIn) -> EmbedOut:
    vec = _simple_encode([in_obj.text])[0]
    return EmbedOut(embedding=vec.tolist(), dim=_dim)


@app.post("/embed/batch", response_model=List[EmbedOut])
def embed_batch(in_obj: EmbedBatchIn) -> List[EmbedOut]:
    vecs = _simple_encode(in_obj.texts)
    return [EmbedOut(embedding=v.tolist(), dim=_dim) for v in vecs]


@app.post("/sentiment", response_model=SentimentOut)
def sentiment(in_obj: SentimentIn) -> SentimentOut:
    # dummy sentiment: length-based pseudo score in [0,1]
    score = min(1.0, max(0.0, len(in_obj.text) / 100.0))
    return SentimentOut(score=score)


@app.post("/rerank", response_model=List[RerankOutItem])
def rerank(in_obj: RerankIn) -> List[RerankOutItem]:
    q_vec = _simple_encode([in_obj.query])[0]
    c_texts = [c.text for c in in_obj.candidates]
    mat = _simple_encode(c_texts)
    # cosine similarity
    def cos(a: np.ndarray, b: np.ndarray) -> float:
        na = np.linalg.norm(a) + 1e-6
        nb = np.linalg.norm(b) + 1e-6
        return float(np.dot(a, b) / (na * nb))

    scored = [RerankOutItem(id=c.id, score=cos(q_vec, v)) for c, v in zip(in_obj.candidates, mat)]
    scored.sort(key=lambda x: x.score, reverse=True)
    return scored


