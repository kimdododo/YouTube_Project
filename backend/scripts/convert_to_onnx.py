"""
m3_korean ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
import os
import sys
from pathlib import Path

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
backend_root = Path(__file__).parent.parent
model_path = backend_root / "models" / "embeddings" / "m3_korean"
output_path = model_path / "model.onnx"

try:
    from transformers import AutoTokenizer, AutoModel
    import torch
    import onnxruntime as ort
except ImportError:
    print("í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜: pip install transformers torch onnxruntime")
    sys.exit(1)

print(f"ğŸ“¦ ëª¨ë¸ ë¡œë“œ: {model_path}")

# transformersë¡œ ì§ì ‘ ë¡œë“œ
print("   - í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(str(model_path), trust_remote_code=True)

print("   - ëª¨ë¸ ë¡œë“œ ì¤‘...")
model = AutoModel.from_pretrained(str(model_path), trust_remote_code=True)
model.eval()

# ë”ë¯¸ ì…ë ¥ ìƒì„± (ONNX ë³€í™˜ìš©)
dummy_input = tokenizer("í…ŒìŠ¤íŠ¸", return_tensors="pt", padding=True, truncation=True)

print(f"ğŸ”„ ONNX ë³€í™˜ ì‹œì‘...")
onnx_path = str(output_path)

# ONNXë¡œ ë‚´ë³´ë‚´ê¸°
torch.onnx.export(
    model,
    (dummy_input["input_ids"], dummy_input["attention_mask"]),
    onnx_path,
    input_names=["input_ids", "attention_mask"],
    output_names=["last_hidden_state", "pooler_output"],
    dynamic_axes={
        "input_ids": {0: "batch_size", 1: "sequence_length"},
        "attention_mask": {0: "batch_size", 1: "sequence_length"},
        "last_hidden_state": {0: "batch_size", 1: "sequence_length"},
        "pooler_output": {0: "batch_size"}
    },
    opset_version=14,
    do_constant_folding=True,
)

print(f"âœ… ë³€í™˜ ì™„ë£Œ: {onnx_path}")

# ë³€í™˜ëœ ëª¨ë¸ ê²€ì¦
try:
    ort_session = ort.InferenceSession(onnx_path)
    print(f"   âœ“ ONNX ëª¨ë¸ ê²€ì¦ ì„±ê³µ")
    print(f"   - ì…ë ¥: {[inp.name for inp in ort_session.get_inputs()]}")
    print(f"   - ì¶œë ¥: {[out.name for out in ort_session.get_outputs()]}")
except Exception as e:
    print(f"   âš ï¸ ONNX ê²€ì¦ ì‹¤íŒ¨: {e}")

