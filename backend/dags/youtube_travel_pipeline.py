"""
YouTube ì—¬í–‰ ê´€ë ¨ ì±„ë„ ë°ì´í„° ìˆ˜ì§‘ ë° ì ì¬ DAG
"""
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from datetime import datetime, timedelta
import sys
import os

# Docker í™˜ê²½: /opt/airflow/utils, ë¡œì»¬ í™˜ê²½: backend/utils
dag_dir = os.path.dirname(os.path.abspath(__file__))
if dag_dir.startswith('/opt/airflow'):
    # Docker í™˜ê²½
    utils_path = '/opt/airflow/utils'
else:
    # ë¡œì»¬ í™˜ê²½
    backend_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    utils_path = os.path.join(backend_root, 'utils')

if utils_path not in sys.path:
    sys.path.insert(0, utils_path)

from youtube_collector import YouTubeCollector
from db_writer import MySQLWriter, BigQueryWriter
import json


# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'data-engineer',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'youtube_travel_pipeline',
    default_args=default_args,
    description='YouTube ì—¬í–‰ ê´€ë ¨ ì±„ë„ ë°ì´í„° ìˆ˜ì§‘ ë° ì ì¬ íŒŒì´í”„ë¼ì¸',
    schedule_interval='0 2 * * *',  # ë§¤ì¼ ì˜¤ì „ 2ì‹œ ì‹¤í–‰
    start_date=days_ago(1),
    catchup=False,
    tags=['youtube', 'travel', 'data-collection'],
)


def _load_channel_list():
    """channel_list.json íŒŒì¼ ë¡œë“œ ë° ê²€ì¦"""
    # channel_list.json íŒŒì¼ ê²½ë¡œ
    dag_dir = os.path.dirname(os.path.abspath(__file__))
    channel_list_path = os.path.join(dag_dir, 'channel_list.json')
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(channel_list_path):
        raise FileNotFoundError(f"channel_list.json not found at {channel_list_path}")
    
    # JSON íŒŒì¼ ì½ê¸° ë° ìœ íš¨ì„± í™•ì¸
    try:
        with open(channel_list_path, 'r', encoding='utf-8') as f:
            channel_list = json.load(f)
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON in channel_list.json: {e}")
    
    # ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
    if not isinstance(channel_list, list):
        raise ValueError(f"channel_list.json must contain a JSON array, got {type(channel_list)}")
    
    # active=Trueì¸ ì±„ë„ë§Œ í•„í„°ë§
    active_channels = [ch for ch in channel_list if ch.get('active', True)]

    # ì°¸ê³  í†µê³„ ë¡œê·¸ ì¶œë ¥
    num_with_id = sum(1 for ch in active_channels if (ch.get('channel_id') or "").strip())
    num_with_handle = sum(1 for ch in active_channels if (ch.get('channel_handle') or "").strip())
    num_with_name = sum(1 for ch in active_channels if (ch.get('name') or "").strip())
    print(
        f"Active channels: {len(active_channels)} | with id: {num_with_id} | "
        f"with handle: {num_with_handle} | with name: {num_with_name}"
    )

    # id/handle/name ì¤‘ í•˜ë‚˜ë¼ë„ ìˆëŠ” ì±„ë„ë§Œ ì‚¬ìš© (ì´ì œ nameë„ í—ˆìš© â†’ ë‚˜ì¤‘ì— nameâ†’id í•´ì„)
    active_channels = [
        ch for ch in active_channels
        if (ch.get('channel_id') or "").strip() or (ch.get('channel_handle') or "").strip() or (ch.get('name') or "").strip()
    ]

    print(f"Loaded {len(active_channels)} active channels from channel_list.json (with id/handle/name)")
    
    return active_channels


def _process_single_channel(ch, api_keys, lock=None):
    """
    ë‹¨ì¼ ì±„ë„ ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)
    
    Args:
        ch: ì±„ë„ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        api_keys: API í‚¤ ë¦¬ìŠ¤íŠ¸
        lock: ìŠ¤ë ˆë“œ ë™ê¸°í™”ìš© Lock (ì„ íƒì )
    
    Returns:
        (success: bool, channel_meta: dict, videos: list, error: str)
    """
    from threading import Lock
    import threading
    
    # ê° ìŠ¤ë ˆë“œë§ˆë‹¤ ë…ë¦½ì ì¸ collector ìƒì„± (API í‚¤ ë¦¬ìŠ¤íŠ¸ëŠ” ê³µìœ )
    collector = YouTubeCollector(api_keys=api_keys)
    
    try:
        # channel_idê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš© (í• ë‹¹ëŸ‰ ì ˆì•½: 1 unit)
        # ì—†ìœ¼ë©´ handle ì‚¬ìš© (í• ë‹¹ëŸ‰ ë§ì´ ì†Œëª¨: 100 units, ì •í™•ë„ ë‚®ìŒ)
        channel_id = (ch.get('channel_id') or "").strip()
        channel_handle = (ch.get('channel_handle') or "").strip()
        
        # identifier ê²°ì •: channel_id ìš°ì„ , ì—†ìœ¼ë©´ handle, ë§ˆì§€ë§‰ìœ¼ë¡œ name í•´ì„
        identifier = None
        if channel_id:
            identifier = channel_id
        elif channel_handle:
            identifier = channel_handle
        else:
            channel_name = (ch.get('name') or "").strip()
            if channel_name:
                try:
                    resolved_id = collector.get_channel_id_by_name(channel_name)
                    if resolved_id:
                        identifier = resolved_id
                except Exception as e:
                    return (False, None, [], f"Name resolution error: {e}")
        
        if not identifier:
            return (False, None, [], "No identifier found (id/handle/name)")
        
        # ì±„ë„ ë°ì´í„° ìˆ˜ì§‘
        bundle = collector.collect_channel_videos(
            channel_id_or_handle=identifier,
            lookback_hours=8760,  # 1ë…„ì¹˜
            max_results=500
        )
        
        meta = bundle["channel_meta"]
        if not meta:
            return (False, None, [], "Failed to get metadata")
        
        # ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€ ì •ë³´ ì£¼ì…
        meta["name"] = ch.get("name", "")
        meta["category"] = ch.get("category", "")
        meta["subscriber_hint"] = ch.get("subscriber_hint", 0)
        
        vids = bundle["videos"]
        for v in vids:
            v["channel_category"] = ch.get("category", "")
            v["channel_name_human"] = ch.get("name", "")
        
        return (True, meta, vids, None)
        
    except Exception as e:
        error_msg = str(e)
        if 'quotaExceeded' in error_msg or 'quota' in error_msg.lower():
            return (False, None, [], "QUOTA_EXCEEDED")
        else:
            return (False, None, [], f"Error: {e}")


def collect_videos(**context):
    """ê° ì±„ë„ì˜ ì¸ê¸° ì˜ìƒ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
    from airflow.models import Variable
    import json
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import threading
    
    print(f"\n{'='*60}")
    print("API í‚¤ ì„¤ì • í™•ì¸")
    print(f"{'='*60}")
    
    # ì—¬ëŸ¬ API í‚¤ ë¡œí…Œì´ì…˜ ì§€ì›
    try:
        api_keys_json = Variable.get("YOUTUBE_API_KEYS", default_var=None)
        print(f"YOUTUBE_API_KEYS Variable ê°’: {api_keys_json[:100] if api_keys_json and len(api_keys_json) > 100 else api_keys_json}")
    except Exception as e:
        print(f"âš ï¸ YOUTUBE_API_KEYS Variable ì½ê¸° ì‹¤íŒ¨: {e}")
        api_keys_json = None
    
    if api_keys_json:
        try:
            # JSON ë¬¸ìì—´ íŒŒì‹±
            api_keys = json.loads(api_keys_json)
            print(f"íŒŒì‹±ëœ API í‚¤ íƒ€ì…: {type(api_keys)}")
            
            if isinstance(api_keys, list) and len(api_keys) > 0:
                print(f"âœ“ Using {len(api_keys)} API keys for rotation")
                # ê° í‚¤ì˜ ì²« 20ìë§Œ í‘œì‹œ
                for i, key in enumerate(api_keys[:3], 1):
                    print(f"  Key {i}: {key[:20]}...")
                if len(api_keys) > 3:
                    print(f"  ... and {len(api_keys) - 3} more keys")
            else:
                print(f"âœ— YOUTUBE_API_KEYS is not a valid list: {type(api_keys)}")
                raise ValueError("YOUTUBE_API_KEYS must be a non-empty JSON array")
        except json.JSONDecodeError as e:
            print(f"âœ— JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print("Warning: YOUTUBE_API_KEYS is invalid JSON, falling back to YOUTUBE_API_KEY")
            try:
                api_key = Variable.get("YOUTUBE_API_KEY", default_var=None)
                if not api_key:
                    raise ValueError("YOUTUBE_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”")
                api_keys = [api_key]
            except Exception as e2:
                print(f"âœ— YOUTUBE_API_KEYë„ ì½ê¸° ì‹¤íŒ¨: {e2}")
                raise ValueError("API í‚¤ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. YOUTUBE_API_KEYS ë˜ëŠ” YOUTUBE_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    else:
        # ë‹¨ì¼ API í‚¤ ì‚¬ìš© (í•˜ìœ„ í˜¸í™˜ì„±)
        print("âš ï¸ YOUTUBE_API_KEYS Variableì´ ì—†ìŠµë‹ˆë‹¤. YOUTUBE_API_KEY ì‚¬ìš© ì‹œë„...")
        try:
            api_key = Variable.get("YOUTUBE_API_KEY", default_var=None)
            if not api_key:
                raise ValueError("YOUTUBE_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”")
            print(f"Using single API key: {api_key[:20]}...")
            api_keys = [api_key]
        except Exception as e:
            print(f"âœ— YOUTUBE_API_KEY ì½ê¸° ì‹¤íŒ¨: {e}")
            raise ValueError("API í‚¤ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. Airflow Variablesì—ì„œ YOUTUBE_API_KEYS (JSON ë°°ì—´) ë˜ëŠ” YOUTUBE_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    
    print(f"{'='*60}\n")
    
    active_channels = _load_channel_list()
    
    all_videos = []
    channels = []
    failed_channels = []
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
    max_workers = min(10, len(api_keys) * 2)  # API í‚¤ ìˆ˜ì— ë¹„ë¡€í•˜ì—¬ ì›Œì»¤ ìˆ˜ ê²°ì • (ìµœëŒ€ 10ê°œ)
    print(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {len(active_channels)}ê°œ ì±„ë„ì„ {max_workers}ê°œ ì›Œì»¤ë¡œ ì²˜ë¦¬")
    print(f"{'='*60}\n")
    
    # ìŠ¤ë ˆë“œ ë™ê¸°í™”ìš© Lock
    lock = threading.Lock()
    
    # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ì±„ë„ì— ëŒ€í•´ ì‘ì—… ì œì¶œ
        future_to_channel = {
            executor.submit(_process_single_channel, ch, api_keys, lock): ch 
            for ch in active_channels
        }
        
        completed = 0
        quota_exhausted = False
        
        # ì™„ë£Œëœ ì‘ì—… ì²˜ë¦¬
        for future in as_completed(future_to_channel):
            ch = future_to_channel[future]
            completed += 1
            
            try:
                success, meta, vids, error = future.result()
                
                if success:
                    channels.append(meta)
                    all_videos.extend(vids)
                    print(f"[{completed}/{len(active_channels)}] âœ“ {ch.get('name')}: {len(vids)} videos")
                else:
                    if error == "QUOTA_EXCEEDED":
                        print(f"[{completed}/{len(active_channels)}] âœ— {ch.get('name')}: API quota exceeded")
                        quota_exhausted = True
                    else:
                        print(f"[{completed}/{len(active_channels)}] âœ— {ch.get('name')}: {error}")
                    failed_channels.append(ch)
                    
            except Exception as e:
                print(f"[{completed}/{len(active_channels)}] âœ— {ch.get('name')}: Exception - {e}")
                failed_channels.append(ch)
            
            # í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ë‚˜ë¨¸ì§€ ì‘ì—… ì·¨ì†Œ (ê°„ë‹¨í•œ ì²´í¬ë¡œ ë³€ê²½)
            if quota_exhausted:
                remaining_futures = [f for f in future_to_channel if not f.done()]
                if remaining_futures:
                    print(f"\nâš ï¸ API quota exhausted. Cancelling {len(remaining_futures)} remaining tasks...")
                    for f in remaining_futures:
                        f.cancel()
                break
    
    print(f"\n{'='*60}")
    print(f"ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ")
    print(f"{'='*60}")
    print(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½:")
    print(f"  - ì‹œë„í•œ ì±„ë„ ìˆ˜: {len(active_channels)}")
    print(f"  - ì„±ê³µí•œ ì±„ë„ ìˆ˜: {len(channels)}")
    print(f"  - ì‹¤íŒ¨í•œ ì±„ë„ ìˆ˜: {len(failed_channels)}")
    print(f"  - ìˆ˜ì§‘ëœ ë¹„ë””ì˜¤ ìˆ˜: {len(all_videos)}")
    print(f"  - ì„±ê³µë¥ : {len(channels)/len(active_channels)*100:.1f}% ({len(channels)}/{len(active_channels)})")
    
    # ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
    if failed_channels:
        print(f"\nğŸ” ì‹¤íŒ¨ ì›ì¸ ë¶„ì„:")
        no_id_no_handle = sum(1 for ch in failed_channels if not (ch.get('channel_id') or "").strip() and not (ch.get('channel_handle') or "").strip())
        has_handle = sum(1 for ch in failed_channels if (ch.get('channel_handle') or "").strip())
        has_id = sum(1 for ch in failed_channels if (ch.get('channel_id') or "").strip())
        print(f"  - channel_id ìˆëŠ” ì±„ë„ ì‹¤íŒ¨: {has_id}ê°œ")
        print(f"  - channel_handleë§Œ ìˆëŠ” ì±„ë„ ì‹¤íŒ¨: {has_handle}ê°œ")
        print(f"  - id/handle ëª¨ë‘ ì—†ëŠ” ì±„ë„ ì‹¤íŒ¨: {no_id_no_handle}ê°œ")
        print(f"\nğŸ’¡ ì‹¤íŒ¨í•œ ì±„ë„ì˜ ëŒ€ë¶€ë¶„ì´ handleë§Œ ìˆëŠ” ê²½ìš°:")
        print(f"     â†’ handle â†’ ID ë³€í™˜ì— 100 units/channel ì†Œëª¨")
        print(f"     â†’ API í• ë‹¹ëŸ‰ ë¶€ì¡±ìœ¼ë¡œ ì‹¤íŒ¨ ê°€ëŠ¥ì„± ë†’ìŒ")
        print(f"     â†’ í•´ê²°: channel_list.jsonì˜ channel_id í•„ë“œ ì±„ìš°ê¸° ê¶Œì¥")
    print(f"\nâœ… ì±„ë„ ë°ì´í„°:")
    if channels:
        print(f"  - {len(channels)}ê°œ ì±„ë„ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ë¨")
        for i, ch in enumerate(channels[:5], 1):  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
            print(f"    {i}. {ch.get('name', 'Unknown')} (ID: {ch.get('id', 'N/A')[:20]}...)")
        if len(channels) > 5:
            print(f"    ... ì™¸ {len(channels) - 5}ê°œ ì±„ë„")
    else:
        print(f"  âš ï¸ ì±„ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
    print(f"\nâœ… ë¹„ë””ì˜¤ ë°ì´í„°:")
    if all_videos:
        print(f"  - {len(all_videos)}ê°œ ë¹„ë””ì˜¤ ìˆ˜ì§‘ë¨")
        # ì±„ë„ë³„ ë¹„ë””ì˜¤ ìˆ˜ ì§‘ê³„
        channel_video_count = {}
        for v in all_videos:
            ch_id = v.get('channel_id', 'unknown')
            channel_video_count[ch_id] = channel_video_count.get(ch_id, 0) + 1
        print(f"  - {len(channel_video_count)}ê°œ ì±„ë„ì—ì„œ ë¹„ë””ì˜¤ ìˆ˜ì§‘ë¨")
        for i, (ch_id, count) in enumerate(list(channel_video_count.items())[:5], 1):
            ch_name = next((ch.get('name', 'Unknown') for ch in channels if ch.get('id') == ch_id), 'Unknown')
            print(f"    {i}. {ch_name}: {count}ê°œ ë¹„ë””ì˜¤")
        if len(channel_video_count) > 5:
            print(f"    ... ì™¸ {len(channel_video_count) - 5}ê°œ ì±„ë„")
    else:
        print(f"  âš ï¸ ë¹„ë””ì˜¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
    
    if len(channels) < len(active_channels):
        missing_count = len(active_channels) - len(channels)
        print(f"\nâš ï¸ {missing_count}ê°œ ì±„ë„ì˜ ìˆ˜ì§‘ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print(f"  â†’ ì„±ê³µë¥ : {len(channels)/len(active_channels)*100:.1f}%")
        
        if missing_count > 0:
            print(f"\n  ê°€ëŠ¥í•œ ì‹¤íŒ¨ ì›ì¸:")
            print(f"    1. API í• ë‹¹ëŸ‰ ì´ˆê³¼ (ê°€ì¥ í”í•œ ì›ì¸)")
            print(f"       â†’ handleë§Œ ìˆëŠ” ì±„ë„ì€ 100 units/channel ì†Œëª¨")
            print(f"       â†’ {len(active_channels)}ê°œ ì±„ë„ Ã— 100 units = {len(active_channels) * 100} units ì´ìƒ í•„ìš”")
            print(f"       â†’ ì¼ì¼ í• ë‹¹ëŸ‰: 10,000 units/API í‚¤")
            print(f"       â†’ í˜„ì¬ API í‚¤: {len(api_keys)}ê°œ = ìµœëŒ€ {len(api_keys) * 10000} units")
            print(f"    2. ì±„ë„ handle í•´ì„ ì‹¤íŒ¨")
            print(f"    3. ì±„ë„ì´ ë¹„í™œì„±í™”ë˜ì—ˆê±°ë‚˜ ì‚­ì œë¨")
            print(f"    4. ë„¤íŠ¸ì›Œí¬ ë˜ëŠ” API ì¼ì‹œì  ì˜¤ë¥˜")
            
            print(f"\n  í•´ê²° ë°©ë²• (ìš°ì„ ìˆœìœ„):")
            print(f"    1. â­ channel_list.jsonì˜ channel_id í•„ë“œ ì±„ìš°ê¸°")
            print(f"       â†’ handle â†’ ID ë³€í™˜ ë¶ˆí•„ìš” â†’ í• ë‹¹ëŸ‰ 1 unit/channelë¡œ ê°ì†Œ")
            print(f"       â†’ fill_channel_ids.py ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© ê¶Œì¥")
            print(f"    2. ë” ë§ì€ API í‚¤ ì¶”ê°€ (í˜„ì¬: {len(api_keys)}ê°œ)")
            print(f"    3. API í‚¤ í• ë‹¹ëŸ‰ ë¦¬ì…‹ ëŒ€ê¸° (ìì • ìë™ ë¦¬ì…‹)")
            print(f"    4. ì‹¤íŒ¨í•œ ì±„ë„ë§Œ ë³„ë„ë¡œ ì¬ì‹œë„")
            
            # ì‹¤íŒ¨í•œ ì±„ë„ ëª©ë¡ ì¶œë ¥ (ì²˜ìŒ 15ê°œ)
            if failed_channels:
                print(f"\n  ì‹¤íŒ¨í•œ ì±„ë„ ëª©ë¡ (ì²˜ìŒ {min(15, len(failed_channels))}ê°œ):")
                for i, ch in enumerate(failed_channels[:15], 1):
                    has_id = "âœ“" if (ch.get('channel_id') or "").strip() else "âœ—"
                    has_handle = "âœ“" if (ch.get('channel_handle') or "").strip() else "âœ—"
                    print(f"    {i:2d}. {ch.get('name', 'Unknown'):20s} | ID: {has_id} | Handle: {has_handle}")
    print(f"{'='*60}\n")
    
    ti = context['ti']
    ti.xcom_push(key='videos', value=all_videos)
    ti.xcom_push(key='channels', value=channels)
    
    # ë¹ˆ ë°ì´í„° ì²´í¬ ë° ê²½ê³ 
    if len(all_videos) == 0:
        print("âš ï¸ WARNING: No videos collected. Possible reasons:")
        print("  1. API quota exceeded")
        print("  2. No videos found in last 7 days")
        print("  3. Channel ID/handle resolution failed")
    
    if len(channels) == 0:
        print("âš ï¸ WARNING: No channels collected.")
    
    return all_videos


def collect_comments(**context):
    """ê° ì˜ìƒì˜ ëŒ“ê¸€ ìˆ˜ì§‘"""
    from airflow.models import Variable
    import json
    
    # ì´ì „ íƒœìŠ¤í¬ì—ì„œ ë¹„ë””ì˜¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    ti = context['ti']
    
    # ë¨¼ì € keyë¡œ ê°€ì ¸ì˜¤ê¸° ì‹œë„
    videos = ti.xcom_pull(task_ids='yt_extract_videos', key='videos')
    
    # ë””ë²„ê¹…: XCom ë°ì´í„° í™•ì¸
    print(f"XCom pull (with key='videos') result type: {type(videos)}")
    if videos is not None:
        print(f"XCom pull result length: {len(videos) if isinstance(videos, list) else 'N/A'}")
    
    # return ê°’ìœ¼ë¡œë„ ì‹œë„ (keyê°€ ì—†ì„ ê²½ìš°)
    if videos is None:
        videos = ti.xcom_pull(task_ids='yt_extract_videos')
        print(f"XCom pull (no key, return value) result type: {type(videos)}")
        if videos is not None and isinstance(videos, list):
            print(f"XCom pull (return value) length: {len(videos)}")
    
    # videosê°€ Noneì´ë©´ ì—ëŸ¬
    if videos is None:
        print("Error: Could not retrieve videos from XCom")
        print("This might indicate that yt_extract_videos task failed")
        raise ValueError("No videos found. Run collect_videos first. Check collect_videos task logs.")
    
    # ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°ëŠ” ì •ìƒ (ë¹„ë””ì˜¤ê°€ ì—†ì„ ìˆ˜ ìˆìŒ)
    if isinstance(videos, list) and len(videos) == 0:
        print("Warning: Videos list is empty. No comments to collect.")
        ti.xcom_push(key='comments', value=[])
        return []
    
    # ì—¬ëŸ¬ API í‚¤ ë¡œí…Œì´ì…˜ ì§€ì› (collect_videosì™€ ë™ì¼í•œ ë°©ì‹)
    api_keys_json = Variable.get("YOUTUBE_API_KEYS", default_var=None)
    if api_keys_json:
        try:
            api_keys = json.loads(api_keys_json)
            if isinstance(api_keys, list) and len(api_keys) > 0:
                print(f"Using {len(api_keys)} API keys for rotation")
                collector = YouTubeCollector(api_keys=api_keys)
            else:
                raise ValueError("YOUTUBE_API_KEYS must be a non-empty JSON array")
        except json.JSONDecodeError:
            api_key = Variable.get("YOUTUBE_API_KEY", default_var=None)
            if not api_key:
                raise ValueError("YOUTUBE_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”")
            collector = YouTubeCollector(api_key=api_key)
    else:
        api_key = Variable.get("YOUTUBE_API_KEY", default_var=None)
        if not api_key:
            raise ValueError("YOUTUBE_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”")
        collector = YouTubeCollector(api_key=api_key)
    print(f"\n{'='*60}")
    print(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘")
    print(f"{'='*60}")
    print(f"ìˆ˜ì§‘í•  ë¹„ë””ì˜¤ ìˆ˜: {len(videos)}")
    
    # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì†ë„ ê°œì„ 
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    def _collect_comments_for_video(video, api_keys):
        """ë‹¨ì¼ ë¹„ë””ì˜¤ì˜ ëŒ“ê¸€ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        # ê° ìŠ¤ë ˆë“œë§ˆë‹¤ ë…ë¦½ì ì¸ collector ìƒì„±
        video_collector = YouTubeCollector(api_keys=api_keys)
        
        video_id = video.get('video_id') or video.get('id')
        video_title = video.get('title', 'Unknown')
        channel_name = video.get('channel_name_human', 'Unknown')
        
        if not video_id:
            return (False, video_title, [], "No video_id")
        
        try:
            comments = video_collector.get_video_comments(video_id, max_results=100)
            return (True, video_title, comments, None)
        except Exception as e:
            error_msg = str(e)
            if 'quotaExceeded' in error_msg or 'quota' in error_msg.lower():
                return (False, video_title, [], "QUOTA_EXCEEDED")
            else:
                return (False, video_title, [], str(e))
    
    all_comments = []
    successful_videos = 0
    failed_videos = 0
    quota_exhausted = False
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì • (ìµœëŒ€ 20ê°œ ì›Œì»¤ë¡œ ëŒ“ê¸€ ìˆ˜ì§‘ ë³‘ë ¬í™”)
    max_workers = min(20, len(api_keys) * 3)
    print(f"ğŸš€ ë³‘ë ¬ ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘: {len(videos)}ê°œ ë¹„ë””ì˜¤ë¥¼ {max_workers}ê°œ ì›Œì»¤ë¡œ ì²˜ë¦¬")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ë¹„ë””ì˜¤ì— ëŒ€í•´ ì‘ì—… ì œì¶œ
        future_to_video = {
            executor.submit(_collect_comments_for_video, video, api_keys): video
            for video in videos
        }
        
        completed = 0
        
        # ì™„ë£Œëœ ì‘ì—… ì²˜ë¦¬
        for future in as_completed(future_to_video):
            video = future_to_video[future]
            completed += 1
            
            try:
                success, video_title, comments, error = future.result()
                
                if success:
                    all_comments.extend(comments)
                    successful_videos += 1
                    if completed % 50 == 0 or len(comments) > 0:
                        channel_name = video.get('channel_name_human', 'Unknown')
                        print(f"[{completed}/{len(videos)}] âœ“ {channel_name} - '{video_title[:30]}...': {len(comments)}ê°œ ëŒ“ê¸€")
                else:
                    failed_videos += 1
                    if error == "QUOTA_EXCEEDED":
                        print(f"[{completed}/{len(videos)}] âœ— QUOTA EXCEEDED: {video_title[:30]}...")
                        quota_exhausted = True
                        # ë‚˜ë¨¸ì§€ ì‘ì—… ì·¨ì†Œ
                        remaining = [f for f in future_to_video if not f.done()]
                        if remaining:
                            print(f"âš ï¸ API í• ë‹¹ëŸ‰ ì´ˆê³¼. {len(remaining)}ê°œ ë¹„ë””ì˜¤ì˜ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘ë‹¨.")
                            for f in remaining:
                                f.cancel()
                        break
                    elif completed % 50 == 0:
                        print(f"[{completed}/{len(videos)}] âœ— Failed: {video_title[:30]}... - {error[:50] if error else 'Unknown error'}")
                        
            except Exception as e:
                failed_videos += 1
                video_title = video.get('title', 'Unknown')
                if completed % 50 == 0:
                    print(f"[{completed}/{len(videos)}] âœ— Exception: {video_title[:30]}... - {str(e)[:50]}")
            
            # ì§„í–‰ë¥  í‘œì‹œ (100ê°œë§ˆë‹¤)
            if completed % 100 == 0:
                print(f"\n  Progress: {completed}/{len(videos)} ({completed/len(videos)*100:.1f}%)")
                print(f"  Success: {successful_videos}, Failed: {failed_videos}")
                print()
    
    print(f"\n{'='*60}")
    print(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
    print(f"{'='*60}")
    print(f"ğŸ“Š ëŒ“ê¸€ ìˆ˜ì§‘ ê²°ê³¼:")
    print(f"  - ì„±ê³µí•œ ë¹„ë””ì˜¤: {successful_videos}/{len(videos)}")
    print(f"  - ì‹¤íŒ¨í•œ ë¹„ë””ì˜¤: {failed_videos}")
    print(f"  - ì´ ìˆ˜ì§‘ëœ ëŒ“ê¸€ ìˆ˜: {len(all_comments)}")
    
    # ë¹„ë””ì˜¤ë³„ ëŒ“ê¸€ ìˆ˜ ì§‘ê³„ (ì²˜ìŒ 10ê°œë§Œ)
    if all_comments:
        video_comment_count = {}
        for comment in all_comments:
            vid_id = comment.get('video_id', 'unknown')
            video_comment_count[vid_id] = video_comment_count.get(vid_id, 0) + 1
        print(f"  - {len(video_comment_count)}ê°œ ë¹„ë””ì˜¤ì—ì„œ ëŒ“ê¸€ ìˆ˜ì§‘ë¨")
        print(f"  - í‰ê·  ë¹„ë””ì˜¤ë‹¹ ëŒ“ê¸€ ìˆ˜: {len(all_comments) / len(video_comment_count):.1f}ê°œ")
    # XComì— ëŒ“ê¸€ ë°ì´í„° ì €ì¥
    ti.xcom_push(key='comments', value=all_comments)
    return all_comments


def load_to_mysql(**context):
    """MySQLì— ë°ì´í„° ì ì¬"""
    # ë¡œì»¬/ë„ì»¤ í™˜ê²½ì—ì„œ ì‰½ê²Œ ë°”ê¿€ ìˆ˜ ìˆë„ë¡ í™˜ê²½ë³€ìˆ˜ë¡œ conn_id ì£¼ì…
    conn_id = os.environ.get('AIRFLOW_MYSQL_CONN_ID', 'mysql_local')
    mysql_writer = MySQLWriter(conn_id=conn_id)
    
    mysql_writer.create_tables()
    
    # ì´ì „ íƒœìŠ¤í¬ë“¤ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    ti = context['ti']
    channels = ti.xcom_pull(task_ids='yt_extract_videos', key='channels')
    videos = ti.xcom_pull(task_ids='yt_extract_videos', key='videos')
    comments = ti.xcom_pull(task_ids='yt_extract_comments', key='comments')
    
    if channels:
        mysql_writer.insert_channels(channels)
    
    if videos:
        # í‚¤ì›Œë“œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 'travel'ë¡œ ì„¤ì • (ì—¬í–‰ ê´€ë ¨ì´ë¯€ë¡œ)
        mysql_writer.insert_videos(videos, keyword='travel')
    
    if comments:
        mysql_writer.insert_comments(comments)
    
    print("Data loaded to MySQL successfully")
    return True


def load_to_bigquery(**context):
    """BigQueryì— ë°ì´í„° ì ì¬"""
    from airflow.models import Variable
    import os
    
    print(f"\n{'='*60}")
    print("BigQuery ì ì¬ ì‹œì‘")
    print(f"{'='*60}")
    
    # í”„ë¡œì íŠ¸ ID ê°€ì ¸ì˜¤ê¸° (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” Airflow Variable)
    try:
        project_id = Variable.get("PROJECT_ID", default_var="eastern-gravity-473301-n8")
    except:
        project_id = os.environ.get('PROJECT_ID', 'eastern-gravity-473301-n8')
    
    try:
        dataset_id = Variable.get("BIGQUERY_DATASET_ID", default_var="youtube_data")
    except:
        dataset_id = "youtube_data"
    
    print(f"Project ID: {project_id}")
    print(f"Dataset ID: {dataset_id}")
    
    # GCP ì¸ì¦ í™•ì¸
    credentials_path = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')
    print(f"GCP Credentials: {credentials_path}")
    
    if credentials_path and not os.path.exists(credentials_path):
        print(f"âš ï¸ Warning: Credentials file not found at {credentials_path}")
    
    # ì´ì „ íƒœìŠ¤í¬ë“¤ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë¨¼ì € ë°ì´í„° í™•ì¸)
    ti = context['ti']
    channels = ti.xcom_pull(task_ids='yt_extract_videos', key='channels')
    videos = ti.xcom_pull(task_ids='yt_extract_videos', key='videos')
    comments = ti.xcom_pull(task_ids='yt_extract_comments', key='comments')
    
    # None ì²´í¬
    if channels is None:
        print("âš ï¸ Warning: channels is None from XCom. Trying return value...")
        channels = []
    
    if videos is None:
        print("âš ï¸ Warning: videos is None from XCom. Trying return value...")
        videos = []
    
    if comments is None:
        print("âš ï¸ Warning: comments is None from XCom. Trying return value...")
        comments = []
    
    print(f"\në°ì´í„° ìš”ì•½:")
    print(f"  Channels: {len(channels)}")
    print(f"  Videos: {len(videos)}")
    print(f"  Comments: {len(comments)}")
    
    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ BigQuery ì ì¬ë¥¼ ê±´ë„ˆëœ€
    if len(channels) == 0 and len(videos) == 0 and len(comments) == 0:
        print(f"\nâš ï¸ WARNING: No data to load to BigQuery!")
        print("This usually means:")
        print("  1. yt_extract_videos task did not collect any data (check its logs)")
        print("  2. API quota might be exceeded")
        print("  3. No videos found in the last 24 hours")
        print("\nSkipping BigQuery load. Check yt_extract_videos and yt_extract_comments task logs.")
        return True
    
    try:
        bq_writer = BigQueryWriter(project_id=project_id, dataset_id=dataset_id)
    except Exception as e:
        print(f"âœ— Failed to initialize BigQueryWriter: {type(e).__name__}: {e}")
        raise
    
    success_count = 0
    
    if channels and len(channels) > 0:
        try:
            bq_writer.load_channels(channels, table_id='travel_channels')
            success_count += 1
        except Exception as e:
            print(f"âœ— Failed to load channels: {e}")
            raise
    
    if videos and len(videos) > 0:
        try:
            bq_writer.load_videos(videos, table_id='travel_videos')
            success_count += 1
        except Exception as e:
            print(f"âœ— Failed to load videos: {e}")
            raise
    
    if comments and len(comments) > 0:
        try:
            bq_writer.load_comments(comments, table_id='travel_comments')
            success_count += 1
        except Exception as e:
            print(f"âœ— Failed to load comments: {e}")
            raise
    
    print(f"\n{'='*60}")
    print(f"âœ“ BigQuery ì ì¬ ì™„ë£Œ ({success_count}/3 í…Œì´ë¸”)")
    print(f"{'='*60}\n")
    return True


# Task ì •ì˜
collect_videos_task = PythonOperator(
    task_id='yt_extract_videos',
    python_callable=collect_videos,
    provide_context=True,
    dag=dag,
)

collect_comments_task = PythonOperator(
    task_id='yt_extract_comments',
    python_callable=collect_comments,
    provide_context=True,
    dag=dag,
)

load_mysql_task = PythonOperator(
    task_id='yt_load_mysql',
    python_callable=load_to_mysql,
    provide_context=True,
    dag=dag,
)

# BigQuery ì ì¬ëŠ” ê¸°ë³¸ ë¹„í™œì„±í™”(ë¡œì»¬ í™˜ê²½). í™˜ê²½ë³€ìˆ˜ë¡œë§Œ ì¼­ë‹ˆë‹¤.
ENABLE_BQ = str(os.environ.get('AIRFLOW_ENABLE_BIGQUERY', 'false')).lower() in ('1', 'true', 'yes')

if ENABLE_BQ:
    load_bigquery_task = PythonOperator(
        task_id='yt_load_bigquery',
        python_callable=load_to_bigquery,
        provide_context=True,
        dag=dag,
    )

# Task ì˜ì¡´ì„± ì„¤ì •
if ENABLE_BQ:
    collect_videos_task >> collect_comments_task >> load_mysql_task >> load_bigquery_task
else:
    collect_videos_task >> collect_comments_task >> load_mysql_task

